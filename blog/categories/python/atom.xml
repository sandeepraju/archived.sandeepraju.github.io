<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: python | Sandeep Raju]]></title>
  <link href="http://sandeepraju.in/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://sandeepraju.in/"/>
  <updated>2014-10-20T11:33:20+00:00</updated>
  <id>http://sandeepraju.in/</id>
  <author>
    <name><![CDATA[Sandeep Raju]]></name>
    <email><![CDATA[me@sandeepraju.in]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[New kid in the block: SHARQ - An Open Source, Rate Limited, Queueing System]]></title>
    <link href="http://sandeepraju.in/blog/2014/10/20/new-kid-in-the-block-sharq-an-open-source-queue/"/>
    <updated>2014-10-20T08:08:00+00:00</updated>
    <id>http://sandeepraju.in/blog/2014/10/20/new-kid-in-the-block-sharq-an-open-source-queue</id>
    <content type="html"><![CDATA[<p>I have been fascinated about task queues since my college days. The way they are designed &amp; work has always interested me. Being a Pythonista, I have been using <a href="">Celery</a> for a long time. <a href="https://twitter.com/asksol">Ask</a> has done an amazing job building Celery and the ecosystem around it. It works great for general use-cases faced by a lot of companies in their day to day workflows (like sending out emails in the background, running periodic tasks, so on). However, sometimes that doesn&rsquo;t suffice.</p>

<!-- more -->


<p>Recently, I faced an unique situation. I had to rate limit the queues in a specific system. Having used Celery for quite a few years, it was my natural choice. But soon, I realized that creating queues dynamically is not possible with Celery. Celery also lacked a clean way to <a href="http://celery.readthedocs.org/en/latest/userguide/tasks.html#Task.rate_limit">update the rate limit of a queue globally</a>. This was not acceptable for my use case.</p>

<p>All my efforts at searching for a queue with this specific feature set (dynamic queues, realtime rate limits &amp; constant flow rate) resulted in scripts and hacks written by several people, around various existing systems that were not built for this use case. It was apparent after some time that there was no simple, out of the box, cleaner solution for this problem and there was a scope for filling this void. This is how SHARQ came into existence.</p>

<p>SHARQ bridges this void in a simple and elegant manner by supporting all the above mentioned features. It is built using <a href="http://www.python.org/">Python</a> (<a href="http://flask.pocoo.org/">Flask</a> &amp; <a href="http://www.gevent.org/">Gevent</a>) and uses <a href="http://redis.io/">Redis</a> as a broker. It talks HTTP with JSON payloads which makes writing workers in any language easy.</p>

<p>SHARQ can be installed from <a href="https://pypi.python.org/pypi">PyPI</a> using the following command:</p>

<p><code>bash
pip install sharq-server
</code></p>

<p>Once SHARQ is installed, it will expose a <strong>sharq-server</strong> command. The command is minimal and accepts a SHARQ configuration file. To get started quickly, grab the SHARQ <a href="https://raw.githubusercontent.com/plivo/sharq-server/master/sharq.conf">sample configuration file from here</a>. The SHARQ Server can be started with the following command:</p>

<p><code>bash
sharq-server --config sharq.conf
</code></p>

<p>and you&rsquo;ll see SHARQ server running in foreground&hellip;</p>

<pre><code>   ___ _              ___    ___
  / __| |_  __ _ _ _ / _ \  / __| ___ _ ___ _____ _ _
  \__ \ ' \/ _` | '_| (_) | \__ \/ -_) '_\ V / -_) '_|
  |___/_||_\__,_|_|  \__\_\ |___/\___|_|  \_/\___|_|

Version: 0.1.0

Listening on: 127.0.0.1:8080
</code></pre>

<p>Now, you can play around with SHARQ using its <a href="http://sharq.io/docs/apireference.html">REST APIs</a>.To know more about how to use SHARQ and its enqueue-dequeue-finish workflow, checkout the <a href="http://sharq.io/docs/gettingstarted.html">getting started section on sharq.io</a>.</p>

<p>SHARQ is open source and is <a href="https://github.com/plivo/sharq-server">available on Github</a>. Pull requests are always welcome!</p>

<p><strong>FURTHER READING</strong>:</p>

<ul>
<li><a href="http://sharq.io/docs">The SHARQ docs</a></li>
<li><a href="https://speakerdeck.com/sandeepraju/sharq-an-open-source-rate-limited-queueing-system-pycon-india-2014">SHARQ at PyCon India 2014</a></li>
<li><a href="https://github.com/sandeepraju/PyCon-India-2014">SHARQ resources of the PyCon talk</a></li>
<li>SHARQ example <a href="https://gist.github.com/sandeepraju/bfa72c7027e1d739b33e">Enqueue</a> and <a href="https://gist.github.com/sandeepraju/3da0ad035aa9bf5504b1">Dequeue</a> gists</li>
<li><a href="https://www.plivo.com/blog/sharq-a-flexible-open-source-rate-limited-queuing-system/">SHARQ on Plivo&rsquo;s blog</a></li>
</ul>


<p><strong>NOTE</strong>: I am the primary author of SHARQ and will be maintaining it constantly with features and bug fixes. SHARQ was written at <a href="https://www.plivo.com/">Plivo</a>. All the views and opinions expressed in this article are my own and does in no way reflect that of Plivo or any other entity.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Fabric To Run Commands Remotely]]></title>
    <link href="http://sandeepraju.in/blog/2013/09/06/using-fabric-to-run-commands-remotely/"/>
    <updated>2013-09-06T05:53:00+00:00</updated>
    <id>http://sandeepraju.in/blog/2013/09/06/using-fabric-to-run-commands-remotely</id>
    <content type="html"><![CDATA[<p>At times, you might have to do some <em>ninjaing</em> around to run commands on mutiple servers. If you are like me, a semi literate in <a href="http://en.wikipedia.org/wiki/Shell_script">shell scripting</a>, you&rsquo;d google for the shell / <a href="http://en.wikipedia.org/wiki/AWK">awk</a> syntax and waste hours writing scripts to do this. But then, wasting hours to run a simple script on multiple servers is pointless. What if I can do this with Python? All problems solved :) That is where <a href="http://docs.fabfile.org/en/1.7/">Fabric</a> comes into picture!</p>

<!-- more -->


<blockquote><p>Fabric is a Python library and command-line tool for streamlining the use of SSH for application deployment or systems administration tasks.</p></blockquote>

<p>Installing fabric is easy. Just run the following commands to install fabric via <a href="https://pypi.python.org/pypi/pip">pip</a>.</p>

<p><code>bash
apt-get install gcc python-dev python-setuptools
pip install fabric
</code></p>

<p>This enables the <em>fab</em> command which will be used to run fabric scripts. Fabric script file is named <code>fabfile.py</code> by convention. So, let us create a small script to check how many users logged into each server.</p>

<p>```python</p>

<h1>filename: fabfile.py</h1>

<p>from fabric.api import env, run</p>

<p>env.hosts = [&lsquo;192.168.0.9&rsquo;, &lsquo;192.168.0.10&rsquo;, &lsquo;192.168.0.11&rsquo;]</p>

<p>def logged_in_users():
    run(&lsquo;who&rsquo;)
```</p>

<p>Now, when we run this function using fabric, an ssh connection is made into each of the servers listed and the function is run. We can run this using the following command,</p>

<p><code>bash
fab logged_in_users
</code></p>

<p>Note that due to the naming convention we followed, we need not even give the filename to the fab command. It will automatically look for the file named <code>fabfile.py</code>    and runs the function you have specified.</p>

<p>Here is what you get as output, the logged in users of each system,</p>

<pre><code>[192.168.0.9] Executing task 'logged_in_users'
[192.168.0.9] run: who
[192.168.0.9] out: sandeep  tty7         2013-09-05 15:24 (:0)
[192.168.0.9] out: alex     pts/0        2013-09-05 22:38 (:0)
[192.168.0.9] out: root     pts/2        2013-09-06 03:31 (192.168.0.9)
[192.168.0.9] out: 

[192.168.0.10] Executing task 'logged_in_users'
[192.168.0.10] run: who
[192.168.0.10] out: sandeep  tty7         2013-09-05 15:22 (:0)
[192.168.0.10] out: 

[192.168.0.11] Executing task 'logged_in_users'
[192.168.0.11] run: who
[192.168.0.11] out: sandeep  tty7         2013-09-05 15:21 (:0)
[192.168.0.11] out: 


Done.
Disconnecting from 192.168.0.11... done.
Disconnecting from 192.168.0.10... done.
Disconnecting from 192.168.0.09... done.
</code></pre>

<p>If you just ran this command, you&rsquo;d realise that the process happens in a sequence. That is, the command is run on each server one after another. To run this command parallely on the listed server, pass the -P flag to the command. So, the same command runs the function in parallelly on each server,</p>

<p><code>bash
fab -P logged_in_users
</code></p>

<p>Now, with fabric, running scripts on multiple servers is simple and quick! :)</p>

<p><strong>SOURCE:</strong> This post is based on <a href="https://gist.github.com/DavidWittman/1886632">A Brief Introduction to Fabric</a> which I stumbled upon when I was looking for getting started with Fabric quickly. Check it out. Its pretty cool :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Django Dumpdata To Dump Selective Rows]]></title>
    <link href="http://sandeepraju.in/blog/2013/08/22/django-dumpdata-to-dump-selective-rows/"/>
    <updated>2013-08-22T08:14:00+00:00</updated>
    <id>http://sandeepraju.in/blog/2013/08/22/django-dumpdata-to-dump-selective-rows</id>
    <content type="html"><![CDATA[<p>Thanks to my <a href="https://twitter.com/GurteshwarSingh">telephony geek friend</a> for letting me know this simple but highly useful method to selectively dump <a href="https://www.djangoproject.com">django</a> models data into a django standard <em>model-dump</em> format. Here is a simple snippet which shows how to selectively dump only the required rows and not the entire model.</p>

<!-- more -->


<p>```python
from django.core import serializers</p>

<h1>Generate a queryset depending on whatever you need</h1>

<p>query_set = YourModel.objects.all()[:2000]  # selecting only first 2k rows</p>

<h1>Serialize the queryset into JSON with indentation of 4 spaces</h1>

<p>json_data = serializers.serialize(&ldquo;json&rdquo;, query_set, indent=4)</p>

<h1>Dump the json data into a file</h1>

<p>output_file = open(&lsquo;YourModel.json&rsquo;, &lsquo;w&rsquo;)
output_file.write(json_data)
output_file.close()</p>

<p>```</p>

<p>Works like a charm!</p>

<p>Happy coding :)</p>
]]></content>
  </entry>
  
</feed>
